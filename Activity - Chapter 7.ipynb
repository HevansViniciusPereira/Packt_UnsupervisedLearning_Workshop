{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Cleaning Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Import the necessary libraries </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import regex\n",
    "import sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Load the LA Times health Twitter data </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>576760256031682561</td>\n",
       "      <td>Sat Mar 14 15:02:15 +0000 2015</td>\n",
       "      <td>Five new running shoes that aim to go the extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>576715414811471872</td>\n",
       "      <td>Sat Mar 14 12:04:04 +0000 2015</td>\n",
       "      <td>Gym Rat: Disq class at Crunch is intense worko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>576438353555365888</td>\n",
       "      <td>Fri Mar 13 17:43:07 +0000 2015</td>\n",
       "      <td>Noshing through thousands of ideas at Natural ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>576438347003908096</td>\n",
       "      <td>Fri Mar 13 17:43:06 +0000 2015</td>\n",
       "      <td>Natural Products Expo also explores beauty, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>576413058177712128</td>\n",
       "      <td>Fri Mar 13 16:02:36 +0000 2015</td>\n",
       "      <td>Free Fitness Weekends in South Bay beach citie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0                               1  \\\n",
       "0  576760256031682561  Sat Mar 14 15:02:15 +0000 2015   \n",
       "1  576715414811471872  Sat Mar 14 12:04:04 +0000 2015   \n",
       "2  576438353555365888  Fri Mar 13 17:43:07 +0000 2015   \n",
       "3  576438347003908096  Fri Mar 13 17:43:06 +0000 2015   \n",
       "4  576413058177712128  Fri Mar 13 16:02:36 +0000 2015   \n",
       "\n",
       "                                                   2  \n",
       "0  Five new running shoes that aim to go the extr...  \n",
       "1  Gym Rat: Disq class at Crunch is intense worko...  \n",
       "2  Noshing through thousands of ideas at Natural ...  \n",
       "3  Natural Products Expo also explores beauty, su...  \n",
       "4  Free Fitness Weekends in South Bay beach citie...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/latimeshealth.txt', sep='|', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name the columns\n",
    "df.columns = ['id', 'datetime', 'tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Run a quick exploratory analysis to ascertain data size and structure </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to look the data structure\n",
    "def dataframe_look(df, nrows):\n",
    "    print(f'SHAPE:\\n{df.shape}')\n",
    "    print(f'COLUMN NAME:\\n{df.columns}')\n",
    "    print(f'HEAD:\\n{df.head(nrows)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE:\n",
      "(4171, 3)\n",
      "COLUMN NAME:\n",
      "Index(['id', 'datetime', 'tweet'], dtype='object')\n",
      "HEAD:\n",
      "                   id                        datetime  \\\n",
      "0  576760256031682561  Sat Mar 14 15:02:15 +0000 2015   \n",
      "1  576715414811471872  Sat Mar 14 12:04:04 +0000 2015   \n",
      "\n",
      "                                               tweet  \n",
      "0  Five new running shoes that aim to go the extr...  \n",
      "1  Gym Rat: Disq class at Crunch is intense worko...  \n"
     ]
    }
   ],
   "source": [
    "dataframe_look(df, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Extract the tweet text and convert it to a list object </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = df['tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADLINES:\n",
      "['Five new running shoes that aim to go the extra mile http://lat.ms/1ELp3wU', 'Gym Rat: Disq class at Crunch is intense workout on pulley system http://lat.ms/1EKOFdr', 'Noshing through thousands of ideas at Natural Products Expo West http://lat.ms/1EHqywg', 'Natural Products Expo also explores beauty, supplements and more http://lat.ms/1EHqyfE', 'Free Fitness Weekends in South Bay beach cities aim to spark activity http://lat.ms/1EH3SMC']\n",
      "LENGTH:\n",
      "4171\n"
     ]
    }
   ],
   "source": [
    "print(f'HEADLINES:\\n{raw[:5]}')\n",
    "print(f'LENGTH:\\n{len(raw)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Write a function to perform language detection and tokenization on white spaces, and then replace the screen names and URLs with SCREENNAME and URL, respectively. The function should also remove punctuation, numbers, and the SCREENNAME and URL replacements. Convert everything to lowercase, except SCREENNAME and URL. It should remove all stop words, perform lemmatization, and keep words with five or more letters only </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify language\n",
    "def lang_ident(txt):\n",
    "    try:\n",
    "       the_language = langdetect.detect(txt)\n",
    "    except:\n",
    "       the_language = 'none'\n",
    "    return the_language\n",
    "\n",
    "# lemmatization\n",
    "def do_lemmatizing(wrd):\n",
    "    out = nltk.corpus.wordnet.morphy(wrd)\n",
    "    return (wrd if out is None else out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_cleaning(txt):\n",
    "    # identify language of tweet\n",
    "    # return null if language not english\n",
    "    lg = lang_ident(txt)\n",
    "    if lg != 'en':\n",
    "        return None\n",
    "    \n",
    "    # split the string on whitespace\n",
    "    out = txt.split(' ')\n",
    "    \n",
    "    # identify screen names\n",
    "    out = ['SCREENNAME' if i.startswith('@') else i for i in out]\n",
    "    \n",
    "    # identify urls\n",
    "    out = [\n",
    "        'URL' if bool(regex.search('http[s]?://', i)) \n",
    "        else i for i in out\n",
    "    ]\n",
    "    \n",
    "    # remove all punctuation\n",
    "    out = [regex.sub('[^\\\\w\\\\s]|\\n', '', i) for i in out]\n",
    "    \n",
    "    # make all non-keywords lowercase\n",
    "    keys = ['SCREENNAME', 'URL']\n",
    "    out = [i.lower() if i not in keys else i for i in out]\n",
    "    \n",
    "    # remove keywords\n",
    "    out = [i for i in out if i not in keys]\n",
    "    \n",
    "    # remove stopwords\n",
    "    list_stop_words = nltk.corpus.stopwords.words('english')\n",
    "    list_stop_words = [regex.sub('[^\\\\w\\\\s]', '', i) for i in list_stop_words]\n",
    "    \n",
    "    out = [i for i in out if i not in list_stop_words]\n",
    "    \n",
    "    # lemmatizing\n",
    "    out = [do_lemmatizing(i) for i in out]\n",
    "    \n",
    "    # keep words 4 or more characters long\n",
    "    out = [i for i in out if len(i) >= 5]\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Apply the function defined in Step 5 to every tweet </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = list(map(tweet_cleaning, raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Remove elements of the output list equal to None </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = list(filter(None.__ne__, clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADLINES:\n",
      "[['running', 'shoes', 'extra'], ['class', 'crunch', 'intense', 'workout', 'pulley', 'system'], ['thousand', 'natural', 'product'], ['natural', 'product', 'explore', 'beauty', 'supplement'], ['fitness', 'weekend', 'south', 'beach', 'spark', 'activity']]\n",
      "\n",
      "LENGTH:\n",
      "4084\n"
     ]
    }
   ],
   "source": [
    "print(f'HEADLINES:\\n{clean[:5]}\\n')\n",
    "print(f'LENGTH:\\n{len(clean)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Turn the elements of each tweet back into a string. Concatenate using white space </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running shoes extra', 'class crunch intense workout pulley system', 'thousand natural product', 'natural product explore beauty supplement', 'fitness weekend south beach spark activity']\n"
     ]
    }
   ],
   "source": [
    "clean_sentences = [' '.join(i) for i in clean]\n",
    "print(clean_sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA and Health Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Specify the number_words, number_docs, and number_features variables </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_words = 10\n",
    "number_docs = 10\n",
    "number_features = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create a bag-of-words model and assign the feature names to another variable for use later on </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer1 = sklearn.feature_extraction.text.CountVectorizer(analyzer=\"word\",\n",
    "                                                              max_df=0.95,\n",
    "                                                              min_df=10,\n",
    "                                                              max_features=number_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 321)\t1\n"
     ]
    }
   ],
   "source": [
    "clean_vec1 = vectorizer1.fit_transform(clean_sentences)\n",
    "print(clean_vec1[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_vec1 = vectorizer1.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Identify the optimal number of topics </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_by_ntopic(data, ntopics):\n",
    "    \n",
    "    output_dict = {\"Number Of Topics\": [], \"Perplexity Score\": []}\n",
    "    \n",
    "    for t in ntopics:\n",
    "        lda = sklearn.decomposition.LatentDirichletAllocation(n_components=t,\n",
    "                                                              learning_method=\"online\",\n",
    "                                                              random_state=0)\n",
    "        \n",
    "        lda.fit(data)\n",
    "        \n",
    "        output_dict[\"Number Of Topics\"].append(t)\n",
    "        output_dict[\"Perplexity Score\"].append(lda.perplexity(data))\n",
    "        \n",
    "    output_df = pd.DataFrame(output_dict)\n",
    "    \n",
    "    index_min_perplexity = output_df[\"Perplexity Score\"].idxmin()\n",
    "    output_num_topics = output_df.loc[\n",
    "        index_min_perplexity,  # index\n",
    "        \"Number Of Topics\"  # column\n",
    "    ]\n",
    "        \n",
    "    return (output_df, output_num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perplexity, optimal_num_topics = perplexity_by_ntopic(clean_vec1,\n",
    "                                                         ntopics=[i for i in range(1, 21) if i % 2 == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number Of Topics  Perplexity Score\n",
      "0                 2        351.847186\n",
      "1                 4        399.638246\n",
      "2                 6        436.261164\n",
      "3                 8        458.095346\n",
      "4                10        476.362744\n",
      "5                12        492.366014\n",
      "6                14        501.756317\n",
      "7                16        514.864902\n",
      "8                18        530.411569\n",
      "9                20        528.501201\n"
     ]
    }
   ],
   "source": [
    "print(df_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Fit the LDA model using the optimal number of topics </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', n_components=2,\n",
       "                          random_state=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = sklearn.decomposition.LatentDirichletAllocation(n_components=optimal_num_topics,\n",
    "                                                      learning_method=\"online\",\n",
    "                                                      random_state=0)\n",
    "\n",
    "lda.fit(clean_vec1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create and print the word-topic table </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(mod, vec, names, docs, ndocs, nwords):\n",
    "    \n",
    "    # word to topic matrix\n",
    "    W = mod.components_\n",
    "    W_norm = W / W.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # topic to document matrix\n",
    "    H = mod.transform(vec)\n",
    "    \n",
    "    W_dict = {}\n",
    "    H_dict = {}\n",
    "    \n",
    "    for tpc_idx, tpc_val in enumerate(W_norm):\n",
    "        topic = f\"Topic{tpc_idx}\"\n",
    "        \n",
    "        # formatting w\n",
    "        W_indices = tpc_val.argsort()[::-1][:nwords]\n",
    "        W_names_values = [\n",
    "            (round(tpc_val[j], 4), names[j]) \n",
    "            for j in W_indices\n",
    "        ]\n",
    "        W_dict[topic] = W_names_values\n",
    "        \n",
    "        # formatting h\n",
    "        H_indices = H[:, tpc_idx].argsort()[::-1][:ndocs]\n",
    "        H_names_values = [\n",
    "            (round(H[:, tpc_idx][j], 4), docs[j]) \n",
    "            for j in H_indices\n",
    "        ]\n",
    "        H_dict[topic] = H_names_values\n",
    "        \n",
    "    W_df = pd.DataFrame(\n",
    "        W_dict, \n",
    "        index=[\"Word\" + str(i) for i in range(nwords)]\n",
    "    )\n",
    "    H_df = pd.DataFrame(\n",
    "        H_dict,\n",
    "        index=[\"Doc\" + str(i) for i in range(ndocs)]\n",
    "    )\n",
    "        \n",
    "    return (W_df, H_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_df, H_df = get_topics(mod=lda,\n",
    "                        vec=clean_vec1,\n",
    "                        names=feature_names_vec1,\n",
    "                        docs=raw,\n",
    "                        ndocs=number_docs,\n",
    "                        nwords=number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Topic0  \\\n",
      "Doc0  (0.9374, RT @skgire: For my non-science friend...   \n",
      "Doc1  (0.9355, Missionaries stricken with Ebola viru...   \n",
      "Doc2  (0.9351, Flu shots reduce the risk of heart at...   \n",
      "Doc3  (0.935, Kids got the biggest boost from this y...   \n",
      "Doc4  (0.9331, Expert panel from @RANDCorporation, @...   \n",
      "Doc5  (0.9318, Eight commonplace ingredients in make...   \n",
      "Doc6          (0.9283, @itsmeyer Ha ha ha, yes indeed!)   \n",
      "Doc7  (0.9283, You can't trust your drug dealer, res...   \n",
      "Doc8                   (0.9277, @nancylayton4 You too!)   \n",
      "Doc9  (0.9268, 11% of hospital patients got care the...   \n",
      "\n",
      "                                                 Topic1  \n",
      "Doc0                   (0.9399, @mycarecircles Thanks!)  \n",
      "Doc1  (0.9385, 74% of doctors surveyed said they hav...  \n",
      "Doc2  (0.9385, Boston Children's Hospital announces ...  \n",
      "Doc3  (0.9373, Little innovations make a big differe...  \n",
      "Doc4  (0.9371, From the Dept. of Happy Accidental Di...  \n",
      "Doc5  (0.937, RT @latimesscience: Study: LAX doesn't...  \n",
      "Doc6  (0.9364, Study: 23% of autism cases may result...  \n",
      "Doc7  (0.9361, Margaret Thatcher's dementia: cause o...  \n",
      "Doc8  (0.9351, One risk of FDA's trans fat ban: \"We ...  \n",
      "Doc9  (0.9332, Look around at 1 and 2-yr-olds. About...  \n"
     ]
    }
   ],
   "source": [
    "print(H_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Print the document-topic table </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Topic0                Topic1\n",
      "Word0       (0.0519, study)       (0.039, latfit)\n",
      "Word1       (0.0216, could)       (0.0328, study)\n",
      "Word2       (0.0204, brain)      (0.0316, health)\n",
      "Word3      (0.0169, report)      (0.0292, cancer)\n",
      "Word4   (0.0157, scientist)      (0.0226, people)\n",
      "Word5      (0.0155, weight)       (0.0167, woman)\n",
      "Word6  (0.0151, california)  (0.0166, researcher)\n",
      "Word7     (0.0133, medical)       (0.0164, death)\n",
      "Word8    (0.0124, research)     (0.0164, patient)\n",
      "Word9    (0.011, treatment)     (0.0161, obesity)\n"
     ]
    }
   ],
   "source": [
    "print(W_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create a biplot visualization </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el909619993336809683916274603\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el909619993336809683916274603_data = {\"mdsDat\": {\"x\": [0.23985488937669625, -0.23985488937669625], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [52.5354130913977, 47.4645869086023]}, \"tinfo\": {\"Term\": [\"latfit\", \"health\", \"cancer\", \"could\", \"people\", \"brain\", \"report\", \"woman\", \"scientist\", \"death\", \"cancer\", \"health\", \"heart\", \"woman\", \"people\", \"death\", \"doctor\", \"healthcare\", \"breast\", \"exercise\", \"latfit\", \"obesity\", \"researcher\", \"study\", \"patient\", \"brain\", \"could\", \"scientist\", \"research\", \"medical\", \"change\", \"california\", \"weight\", \"report\", \"eating\", \"treatment\", \"study\"], \"Freq\": [242.0, 195.0, 179.0, 120.0, 139.0, 114.0, 94.0, 103.0, 87.0, 101.0, 179.33697310498826, 194.53278789023523, 96.87904687979135, 102.78501309847267, 138.8051897548019, 101.12172909714018, 98.0556213267714, 85.11229855749046, 56.850386189363505, 57.44089217005841, 239.92006469010892, 98.77463906495603, 102.3460546256656, 201.5830806598512, 101.07746748056958, 113.43935058351103, 120.1445315674891, 87.30974118908833, 68.91631315112922, 73.80806173767697, 60.03711368760748, 84.06760314069956, 85.92732128946065, 93.75748027585854, 47.095384317538404, 61.33801305830568, 288.29134966827615], \"Total\": [242.0, 195.0, 179.0, 120.0, 139.0, 114.0, 94.0, 103.0, 87.0, 101.0, 179.96330829523532, 195.248072058387, 97.4412266173208, 103.38612874002354, 139.735032913755, 101.80711520354353, 98.78219756033846, 85.76412739009574, 57.41050527249206, 58.02874979343547, 242.58736238981135, 100.6211064085962, 105.04587877054946, 489.87443032812735, 127.35295168627394, 114.0418167178838, 120.88045265337746, 87.9006428572172, 69.57803626905962, 74.53037841742938, 60.632782157996594, 84.90840284199066, 86.81950033991886, 94.76909633512622, 47.64553298947969, 62.794267673685546, 489.87443032812735], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"logprob\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.5352, -3.4538, -4.151, -4.0918, -3.7914, -4.1081, -4.1389, -4.2805, -4.684, -4.6737, -3.2441, -4.1316, -4.0961, -3.4182, -4.1085, -3.8917, -3.8342, -4.1535, -4.39, -4.3215, -4.528, -4.1913, -4.1694, -4.0822, -4.7708, -4.5065, -2.959], \"loglift\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.6402, 0.64, 0.6379, 0.6379, 0.637, 0.6369, 0.6363, 0.6361, 0.6339, 0.6335, 0.6326, 0.6252, 0.6176, -0.2443, 0.4126, 0.7399, 0.7391, 0.7384, 0.7356, 0.7354, 0.7353, 0.7352, 0.7349, 0.7345, 0.7336, 0.7217, 0.215]}, \"token.table\": {\"Topic\": [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2], \"Freq\": [0.008768713343753511, 0.9908646078441468, 0.9928496488483485, 0.017418414892076288, 0.011777397366205778, 0.9893013787612853, 0.9946471961181388, 0.005556688246470049, 0.01649272826363476, 0.9895636958180858, 0.008272636129742847, 0.9927163355691416, 0.9920721139978295, 0.009822496178196331, 0.9920815938533796, 0.010123281569932445, 0.020988326444386796, 0.9864513428861793, 0.982271722256683, 0.017232837232573385, 0.9987294519440232, 0.0051216894971488364, 0.9910903612810036, 0.011659886603305924, 0.9954718692217042, 0.010262596589914475, 0.98933430676552, 0.012366678834569001, 0.013417347680689409, 0.9928837283710162, 0.9838890023529128, 0.019876545502079047, 0.7930715280852477, 0.20415702703184596, 0.9947398093489651, 0.0071564015061076625, 0.010551963020347482, 0.9918845239126634, 0.014372351587115516, 0.9916922595109706, 0.9710043001572433, 0.0285589500046248, 0.011376481075620412, 0.9897538535789757, 0.4123505688278045, 0.5879057614970679, 0.015925020500224072, 0.9714262505136684, 0.01151814967933199, 0.9905608724225511, 0.996265178465145, 0.00967247746082665], \"Term\": [\"brain\", \"brain\", \"breast\", \"breast\", \"california\", \"california\", \"cancer\", \"cancer\", \"change\", \"change\", \"could\", \"could\", \"death\", \"death\", \"doctor\", \"doctor\", \"eating\", \"eating\", \"exercise\", \"exercise\", \"health\", \"health\", \"healthcare\", \"healthcare\", \"heart\", \"heart\", \"latfit\", \"latfit\", \"medical\", \"medical\", \"obesity\", \"obesity\", \"patient\", \"patient\", \"people\", \"people\", \"report\", \"report\", \"research\", \"research\", \"researcher\", \"researcher\", \"scientist\", \"scientist\", \"study\", \"study\", \"treatment\", \"treatment\", \"weight\", \"weight\", \"woman\", \"woman\"]}, \"R\": 10, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el909619993336809683916274603\", ldavis_el909619993336809683916274603_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el909619993336809683916274603\", ldavis_el909619993336809683916274603_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el909619993336809683916274603\", ldavis_el909619993336809683916274603_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_plot = pyLDAvis.sklearn.prepare(lda, clean_vec1, vectorizer1, R=10)\n",
    "pyLDAvis.display(lda_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create the appropriate bag-of-words model and output the feature names as another variable </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = sklearn.feature_extraction.text.TfidfVectorizer(analyzer=\"word\",\n",
    "                                                              max_df=0.5,\n",
    "                                                              min_df=20,\n",
    "                                                              max_features=number_features,\n",
    "                                                              smooth_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_vec2 = vectorizer2.fit_transform(clean_sentences)\n",
    "print(clean_vec2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_vec2 = vectorizer2.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Define and fit the NMF algorithm using the number of topics </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.1, init='nndsvda', l1_ratio=0.5, n_components=2, random_state=0,\n",
       "    solver='mu')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf = sklearn.decomposition.NMF(n_components=optimal_num_topics,\n",
    "                                init=\"nndsvda\",\n",
    "                                solver=\"mu\",\n",
    "                                beta_loss=\"frobenius\",\n",
    "                                random_state=0,\n",
    "                                alpha=0.1,\n",
    "                                l1_ratio=0.5)\n",
    "nmf.fit(clean_vec2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Get the topic-document and word-topic tables. Take a few minutes to explore the word groupings and try to define the abstract topics. Can you quantify the meanings of the word groupings? Do the word groupings make sense? Are the results similar to those produced using LDA? </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_df, H_df = get_topics(mod=nmf,\n",
    "                        vec=clean_vec2,\n",
    "                        names=feature_names_vec2,\n",
    "                        docs=raw,\n",
    "                        ndocs=number_docs,\n",
    "                        nwords=number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Topic0                Topic1\n",
      "Word0    (0.3735, study)      (0.5947, latfit)\n",
      "Word1    (0.026, cancer)       (0.0485, steps)\n",
      "Word2   (0.0208, people)       (0.0444, today)\n",
      "Word3   (0.0186, health)      (0.04, exercise)\n",
      "Word4  (0.0185, obesity)  (0.0272, healthtips)\n",
      "Word5    (0.0182, brain)     (0.0255, workout)\n",
      "Word6  (0.0174, suggest)      (0.022, fitness)\n",
      "Word7   (0.0168, weight)     (0.0202, getting)\n",
      "Word8    (0.0154, woman)       (0.0142, great)\n",
      "Word9    (0.0131, death)     (0.0131, morning)\n"
     ]
    }
   ],
   "source": [
    "print(W_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Topic0  \\\n",
      "Doc0  (0.2031, Move over FTO. Scientists now say the...   \n",
      "Doc1  (0.2031, Researchers comb through measurements...   \n",
      "Doc2  (0.2031, The world's oldest, most widely disse...   \n",
      "Doc3  (0.2031, Life with #breastcancer, from @nbcpar...   \n",
      "Doc4  (0.2031, Understanding healthcare reform: A pi...   \n",
      "Doc5  (0.2031, When \"free\" actually means \"really ex...   \n",
      "Doc6  (0.2031, Docs' intensive 'get healthy' program...   \n",
      "Doc7  (0.2031, FDA approves first drug treatment for...   \n",
      "Doc8  (0.2031, Try This! A ballet-inspired 'booty li...   \n",
      "Doc9  (0.2031, RT @latimesscience: Does your lettuce...   \n",
      "\n",
      "                                                 Topic1  \n",
      "Doc0  (0.2272, There are laws that protect kids from...  \n",
      "Doc1  (0.2272, Farm subsidies from the government ar...  \n",
      "Doc2  (0.2272, Did you know baked beans, soy sauce a...  \n",
      "Doc3  (0.2272, Obama administration delays the healt...  \n",
      "Doc4  (0.2272, Study finds chronic brain damage in r...  \n",
      "Doc5  (0.2272, RT @lacrepat: For what it's worth, #f...  \n",
      "Doc6  (0.2272, RT @latimesscience: Studying concussi...  \n",
      "Doc7  (0.2272, RT @MyLastBite: Starting #TheWire sea...  \n",
      "Doc8  (0.2272, RT @kurtissl: #10000steps #LATFIT - J...  \n",
      "Doc9  (0.2272, RT @tennyt: We're eating 1,000% more ...  \n"
     ]
    }
   ],
   "source": [
    "print(H_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
